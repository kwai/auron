name: TPC-DS Reusable

on:
  workflow_call:
    inputs:
      sparkver:
        required: true
        type: string
      sparkurl:
        required: true
        type: string
      javaver:
        required: false
        type: string
        default: '8'
      celebornver:
        required: false
        type: string
        default: ''
      celebornurl:
        required: false
        type: string
        default: ''
      unifflever:
        required: false
        type: string
        default: ''
      uniffleurl:
        required: false
        type: string
        default: ''
      hadoopver:
        required: false
        type: string
        default: ''
      hadoopurl:
        required: false
        type: string
        default: ''
      extrablazebuildopt:
        required: false
        type: string
        default: ''
      extrablazeidentifier:
        required: false
        type: string
        default: ''
      extrasparkconf:
        required: false
        type: string
        default: ''
      queries:
        required: false
        type: string
        default: |
          [
            "q1,q2,q3,q4,q5,q6,q7,q8,q9",
            "q10,q11,q12,q13,q14a,q14b,q15,q16,q17,q18,q19",
            "q20,q21,q22,q23a,q23b,q24a,q24b,q25,q26,q27,q28,q29",
            "q31,q33,q34,q35,q36,q37,q38,q39a,q39b",
            "q40,q41,q42,q43,q44,q45,q46,q47,q48,q49",
            "q50,q51,q52,q53,q54,q55,q56,q57,q58,q59",
            "q60,q61,q62,q63,q64,q65,q66,q67,q68,q69",
            "q70,q71,q72,q73,q74,q75,q76,q77,q78,q79",
            "q80,q81,q82,q83,q84,q85,q86,q87,q88,q89",
            "q90,q91,q92,q93,q94,q95,q96,q97,q98,q99"
          ]

jobs:
  build-validator:
    name: Build Validator
    runs-on: ubuntu-latest
    steps:
      - uses: actions/cache@v4
        id: cache-tpcds-validator
        with:
          key: tpcds-validator
          path: target/tpcds-validator_2.12-0.1.0-SNAPSHOT-with-dependencies.jar

      - uses: actions/checkout@v4
        if: steps.cache-tpcds-validator.outputs.cache-hit != 'true'
        with:
          repository: blaze-init/tpcds-validator

      - uses: actions/setup-java@v4
        if: steps.cache-tpcds-validator.outputs.cache-hit != 'true'
        with:
          distribution: 'adopt-hotspot'
          java-version: ${{ inputs.javaver }}
          cache: 'maven'

      - name: Build
        if: steps.cache-tpcds-validator.outputs.cache-hit != 'true'
        run: mvn package -DskipTests

      - name: Upload Artifact
        uses: actions/upload-artifact@v4
        with:
          name: tpcds-validator-${{ inputs.sparkver }}-jdk-${{ inputs.javaver }}${{ inputs.extrablazeidentifier }}
          path: target/tpcds-validator_2.12-0.1.0-SNAPSHOT-with-dependencies.jar
          overwrite: true

  build-blaze-jar:
    name: Build Blaze JAR
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-java@v4
        with:
          distribution: 'adopt-hotspot'
          java-version: ${{ inputs.javaver }}
          cache: 'maven'

      - uses: arduino/setup-protoc@v2
        with:
          version: "21.7"
          repo-token: ${{ secrets.GITHUB_TOKEN }}

      - uses: actions-rust-lang/setup-rust-toolchain@v1
        with:
          toolchain: nightly
          rustflags: --allow warnings -C target-feature=+aes
          components:
            cargo
            rustfmt

      - name: Rustfmt Check
        uses: actions-rust-lang/rustfmt@v1

      - name: Cargo test
        run: cargo +nightly test --workspace --all-features

      - name: Build ${{ inputs.sparkver }} jdk-${{ inputs.javaver }}
        run: ./build/mvn package -Ppre -P${{ inputs.sparkver }} -Pjdk-${{ inputs.javaver }} ${{ inputs.extrablazebuildopt }}

      - name: Upload ${{ inputs.sparkver }} jdk-${{ inputs.javaver }}
        uses: actions/upload-artifact@v4
        with:
          name: blaze-engine-${{ inputs.sparkver }}-jdk-${{ inputs.javaver }}${{ inputs.extrablazeidentifier }}
          path: target/blaze-engine-${{ inputs.sparkver }}-pre-*-SNAPSHOT.jar
          overwrite: true

  run-tpcds-test:
    name: Run TPC-DS test ${{ matrix.query }}
    needs: [build-validator, build-blaze-jar]
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        query: ${{ fromJson(inputs.queries) }}
    steps:
      - uses: actions/checkout@v4

      - uses: actions/cache@v4
        id: cache-spark-bin
        with:
          path: spark-bin-${{ inputs.sparkver }}
          key: spark-bin-${{ inputs.sparkver }}

      - name: Setup ${{ inputs.sparkver }}
        id: setup-spark-bin
        if: steps.cache-spark-bin.outputs.cache-hit != 'true'
        run: |
          wget -c ${{ inputs.sparkurl }}
          mkdir -p spark-bin-${{ inputs.sparkver }}
          cd spark-bin-${{ inputs.sparkver }} && tar -xf ../spark-*.tgz --strip-component=1

      - uses: actions/download-artifact@v4
        with:
          name: blaze-engine-${{ inputs.sparkver }}-jdk-${{ inputs.javaver }}${{ inputs.extrablazeidentifier }}

      - uses: actions/download-artifact@v4
        with:
          name: tpcds-validator-${{ inputs.sparkver }}-jdk-${{ inputs.javaver }}${{ inputs.extrablazeidentifier }}

      - name: Checkout TPC-DS Data
        uses: actions/checkout@v4
        with:
          repository: blaze-init/tpcds_1g
          path: dev/tpcds_1g

      - name: Install Blaze JAR
        run: |
          ls -la
          cp blaze-engine-*${{ inputs.sparkver }}*.jar spark-bin-${{ inputs.sparkver }}/jars/

      - uses: actions/setup-java@v4
        with:
          distribution: 'adopt-hotspot'
          java-version: ${{ inputs.javaver }}
          cache: 'maven'

      - uses: actions/cache@v4
        if: ${{ inputs.celebornver != '' && inputs.celebornurl != '' }}
        id: cache-celeborn-bin
        with:
          path: celeborn-bin-${{ inputs.celebornver }}
          key: celeborn-bin-${{ inputs.celebornver }}

      - name: Setup Celeborn-${{ inputs.celebornver }}
        id: setup-celeborn-bin
        if: ${{ inputs.celebornver != '' && inputs.celebornurl != '' &&  steps.cache-celeborn-bin.outputs.cache-hit != 'true' }}
        run: |
          wget -c ${{ inputs.celebornurl }} && \
          mkdir -p celeborn-bin-${{ inputs.celebornver }} && \
          cd celeborn-bin-${{ inputs.celebornver }} && tar -xf ../apache-celeborn-*.tgz --strip-component=1

      - name: Start Celeborn-${{ inputs.celebornver }}
        if: ${{ inputs.celebornver != '' && inputs.celebornurl != '' }}
        run: |
          mkdir -p /tmp/rss/data && mkdir -p /tmp/rss/logs && \
          cd celeborn-bin-${{ inputs.celebornver }} && \
          bash -c "echo -e 'CELEBORN_MASTER_MEMORY=4g\nCELEBORN_WORKER_MEMORY=4g\nCELEBORN_WORKER_OFFHEAP_MEMORY=8g\nCELEBORN_LOG_DIR=/tmp/rss/logs' > ./conf/celeborn-env.sh" && \
          bash -c "echo -e 'celeborn.worker.storage.dirs /tmp/rss\nceleborn.worker.storage.workingDir data\nceleborn.worker.commitFiles.threads 128\nceleborn.worker.sortPartition.threads 64' > ./conf/celeborn-defaults.conf" && \
          bash ./sbin/start-master.sh && \
          bash ./sbin/start-worker.sh

      - uses: actions/cache@v4
        if: ${{ inputs.unifflever != '' && inputs.uniffleurl != '' }}
        id: cache-uniffle-bin
        with:
          path: uniffle-bin-${{ inputs.unifflever }}
          key: uniffle-bin-${{ inputs.unifflever }}

      - name: Setup Uniffle-${{ inputs.unifflever }}
        id: setup-uniffle-bin
        if: ${{ inputs.unifflever != '' && inputs.uniffleurl != '' &&  steps.cache-uniffle-bin.outputs.cache-hit != 'true' }}
        run: |
          wget -c ${{ inputs.uniffleurl }} && \
          mkdir -p uniffle-bin-${{ inputs.unifflever }} && \
          tar -xf ./apache-uniffle-${{ inputs.unifflever }}-incubating-bin.tar.gz -C uniffle-bin-${{ inputs.unifflever }} --strip-component=1

      - uses: actions/cache@v4
        if: ${{ inputs.hadoopver != '' && inputs.hadoopurl != '' }}
        id: cache-hadoop-bin
        with:
          path: hadoop-bin-${{ inputs.hadoopver }}
          key: hadoop-bin-${{ inputs.hadoopver }}

      - name: Setup hadoop-${{ inputs.hadoopver }}
        id: setup-hadoop-bin
        if: ${{ inputs.hadoopver != '' && inputs.hadoopurl != '' &&  steps.cache-hadoop-bin.outputs.cache-hit != 'true' }}
        run: |
          wget -c ${{ inputs.hadoopurl }} && \
          mkdir -p hadoop-bin-${{ inputs.hadoopver }} && \
          tar -xf ./hadoop-${{ inputs.hadoopver }}.tar.gz -C hadoop-bin-${{ inputs.hadoopver }} --strip-component=1  && \
          ls -la hadoop-bin-${{ inputs.hadoopver }}

      - name: Start Uniffle-${{ inputs.unifflever }}
        if: ${{ inputs.unifflever != '' && inputs.uniffleurl != '' }}
        run: |
          mkdir -p /tmp/rss/data && mkdir -p /tmp/rss/logs && \
          cd uniffle-bin-${{ inputs.unifflever }} && \
          bash -c "echo -e 'XMX_SIZE=16g\nHADOOP_HOME=../hadoop-bin-${{ inputs.hadoopver }}\nRSS_LOG_DIR=/tmp/rss/logs' > ./bin/rss-env.sh" && \
          bash -c "echo -e 'rss.coordinator.shuffle.nodes.max 1\nrss.rpc.server.port 19999' > ./conf/coordinator.conf" && \
          bash -c "echo -e 'rss.server.app.expired.withoutHeartbeat 7200000\nrss.server.heartbeat.delay 3000\nrss.rpc.server.port 19997\nrss.rpc.server.type GRPC_NETTY\nrss.jetty.http.port 19996\nrss.server.netty.port 19995\nrss.storage.basePath /tmp/rss/data\nrss.storage.type MEMORY_LOCALFILE\nrss.coordinator.quorum localhost:19999\nrss.server.flush.thread.alive 10\nrss.server.single.buffer.flush.threshold 64m' > ./conf/server.conf" && \
          bash ./bin/start-coordinator.sh && bash ./bin/start-shuffle-server.sh

      - name: Run
        run: |
          ls -la
          export RUST_LOG=ERROR
          export RUST_BACKTRACE=1
          export SPARK_PRINT_LAUNCH_COMMAND=1
          SPARK_HOME=spark-bin-${{ inputs.sparkver }} dev/run-tpcds-test \
            ${{ inputs.extrasparkconf }} \
            --data-location dev/tpcds_1g \
            --query-filter ${{ matrix.query }}

      - name: Upload RSS log
        if: ${{ failure() && (inputs.celebornver != '' || inputs.unifflever != '') }}
        uses: actions/upload-artifact@v4
        with:
          name: rss-log-${{ inputs.extrablazeidentifier }}
          path: |
            /tmp/rss/logs/*
